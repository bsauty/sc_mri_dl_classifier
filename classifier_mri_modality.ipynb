{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIDS Loader\n",
    "\n",
    "\n",
    "We use the data in `SpineGeneric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bids_neuropoly import bids\n",
    "from medicaltorch import datasets as mt_datasets\n",
    "from medicaltorch import transforms as mt_transforms\n",
    "from medicaltorch import filters as mt_filters\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class SliceFilter(mt_filters.SliceFilter):\n",
    "    \"\"\"This class extends the SliceFilter that already\n",
    "    filters for empty labels and inputs. It will filter\n",
    "    slices that has only zeros after cropping. To avoid\n",
    "    feeding empty inputs into the network.\n",
    "    \"\"\"\n",
    "    def __call__(self, sample):\n",
    "        super_ret = super().__call__(sample)\n",
    "\n",
    "        # Already filtered by base class\n",
    "        if not super_ret:\n",
    "            return super_ret\n",
    "\n",
    "        # Filter slices where there are no values after cropping\n",
    "        input_img = Image.fromarray(sample['input'], mode='F')\n",
    "        input_cropped = transforms.functional.center_crop(input_img, (128, 128))\n",
    "        input_cropped = np.array(input_cropped)\n",
    "        count = np.count_nonzero(input_cropped)\n",
    "\n",
    "        if count <= 0:\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "    \n",
    "class BIDSSegPair2D(mt_datasets.SegmentationPair2D):\n",
    "    def __init__(self, input_filename, gt_filename, metadata):\n",
    "        super().__init__(input_filename, gt_filename)\n",
    "        self.metadata = metadata\n",
    "        self.metadata[\"input_filename\"] = input_filename\n",
    "        self.metadata[\"gt_filename\"] = gt_filename\n",
    "\n",
    "    def get_pair_slice(self, slice_index, slice_axis=2):\n",
    "        dreturn = super().get_pair_slice(slice_index, slice_axis)\n",
    "        self.metadata[\"slice_index\"] = slice_index\n",
    "        dreturn[\"input_metadata\"][\"bids_metadata\"] = self.metadata\n",
    "        return dreturn\n",
    "\n",
    "\n",
    "class MRI2DBidsSegDataset(mt_datasets.MRI2DSegmentationDataset):\n",
    "    def _load_filenames(self):\n",
    "        for input_filename, gt_filename, bids_metadata in self.filename_pairs:\n",
    "            segpair = BIDSSegPair2D(input_filename, gt_filename,\n",
    "                                    bids_metadata)\n",
    "            self.handlers.append(segpair)\n",
    "\n",
    "\n",
    "class BidsDataset(MRI2DBidsSegDataset):\n",
    "    def __init__(self, root_dir, slice_axis=2, cache=True,\n",
    "                 transform=None, slice_filter_fn=None,\n",
    "                 canonical=False, labeled=True):\n",
    "        self.bids_ds = bids.BIDS(root_dir)\n",
    "        self.filename_pairs = []\n",
    "        self.metadata = {\"FlipAngle\": [], \"RepetitionTime\": [], \"EchoTime\": [], \"Manufacturer\": []}\n",
    "\n",
    "        for subject in self.bids_ds.get_subjects():\n",
    "\n",
    "            if not subject.has_derivative(\"labels\"):\n",
    "                print(\"Subject without derivative, skipping.\")\n",
    "                continue\n",
    "            derivatives = subject.get_derivatives(\"labels\")\n",
    "            cord_label_filename = None\n",
    "\n",
    "            for deriv in derivatives:\n",
    "                if deriv.endswith(\"seg-manual.nii.gz\"):\n",
    "                    cord_label_filename = deriv\n",
    "\n",
    "            if cord_label_filename is None:\n",
    "                continue\n",
    "\n",
    "            if not subject.has_metadata():\n",
    "                print(\"Subject without metadata.\")\n",
    "                continue\n",
    "\n",
    "            metadata = subject.metadata()\n",
    "            \n",
    "\n",
    "            self.filename_pairs.append((subject.record.absolute_path,\n",
    "                                        cord_label_filename, metadata))\n",
    "\n",
    "        super().__init__(self.filename_pairs, slice_axis, cache,\n",
    "                         transform, slice_filter_fn, canonical)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Here we define the architecture of the network in a PyTorch Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DownConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, drop_rate=0.2, bn_momentum=0.1):\n",
    "        super(DownConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_bn = nn.BatchNorm2d(out_ch, momentum=bn_momentum)\n",
    "        self.conv_drop = nn.Dropout2d(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = self.conv_bn(x)\n",
    "        x = self.conv_drop(x)\n",
    "        return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, drop_rate=0.2, bn_momentum=0.1):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = DownConv(1, 32, drop_rate, bn_momentum)\n",
    "        self.mp1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2 = DownConv(32, 32, drop_rate, bn_momentum)\n",
    "        self.mp2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv3 = DownConv(32, 64, drop_rate, bn_momentum)\n",
    "        self.mp3 = nn.MaxPool2d(2)       \n",
    "        \n",
    "        self.flat = Flatten()\n",
    "        self.dense1 = nn.Linear(16384, 512)\n",
    "        self.drop = nn.Dropout2d(drop_rate)\n",
    "        self.dense2 = nn.Linear(512, 6)\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.mp1(x1)\n",
    "        \n",
    "        x3 = self.conv2(x2)\n",
    "        x4 = self.mp2(x3)\n",
    "        \n",
    "        x5 = self.conv3(x4)\n",
    "        x6 = self.mp3(x5)\n",
    "        \n",
    "        x7 = self.flat(x6)\n",
    "        x8 = F.relu(self.dense1(x7))\n",
    "        x9 = self.drop(x8)\n",
    "        x10 = self.dense2(x9)\n",
    "        x11 = self.soft(x10)\n",
    "\n",
    "        return(x11)\n",
    "\n",
    "#torch tensors are of the format (batch_size, n_channels, shape_of_image)\n",
    "a = torch.rand(18,1,128,128)\n",
    "test = Classifier().forward(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "import pickle\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "\n",
    "\n",
    "\n",
    "def get_modality(batch):\n",
    "    labels = []\n",
    "    for acq in batch['input_metadata']:\n",
    "        path = acq.__getitem__('bids_metadata')['input_filename']\n",
    "        name = os.path.basename(path)\n",
    "        if \"acq-MToff_MTS\" in name :\n",
    "            labels.append(0)\n",
    "            continue\n",
    "        if \"acq-MTon_MTS\" in name :\n",
    "            labels.append(1)\n",
    "            continue\n",
    "        if \"acq-T1w_MTS\" in name :\n",
    "            labels.append(2)\n",
    "            continue\n",
    "        if \"T1w\" in name :\n",
    "            labels.append(3)\n",
    "            continue\n",
    "        if \"T2star\" in name :\n",
    "            labels.append(4)\n",
    "            continue\n",
    "        if \"T2w\" in name :\n",
    "            labels.append(5) \n",
    "            continue\n",
    "    return labels\n",
    "\n",
    "def OneHotEncode(labels):\n",
    "    ohe_labels = []\n",
    "    for label in labels :\n",
    "        ohe = [0 for i in range(6)]\n",
    "        ohe[label] = 1 \n",
    "        ohe_labels.append(ohe)\n",
    "    return torch.cuda.FloatTensor(ohe_labels)\n",
    "\n",
    "\n",
    "def cmd_train(context):\n",
    "    \"\"\"Main command do train the network.\n",
    "    :param context: this is a dictionary with all data from the\n",
    "                    configuration file:\n",
    "                        - 'command': run the specified command (e.g. train, test)\n",
    "                        - 'gpu': ID of the used GPU\n",
    "                        - 'bids_path_train': list of relative paths of the BIDS folders of each training center\n",
    "                        - 'bids_path_validation': list of relative paths of the BIDS folders of each validation center\n",
    "                        - 'bids_path_test': list of relative paths of the BIDS folders of each test center\n",
    "                        - 'batch_size'\n",
    "                        - 'dropout_rate'\n",
    "                        - 'batch_norm_momentum'\n",
    "                        - 'num_epochs'\n",
    "                        - 'initial_lr': initial learning rate\n",
    "                        - 'log_directory': folder name where log files are saved\n",
    "                        - 'debugging': allows extended verbosity and intermediate outputs\n",
    "    \"\"\"\n",
    "    # Set the GPU\n",
    "    gpu_number = int(0)\n",
    "    torch.cuda.set_device(gpu_number)\n",
    "\n",
    "    # These are the training transformations\n",
    "    train_transform = transforms.Compose([\n",
    "        mt_transforms.CenterCrop2D((128, 128)),\n",
    "        mt_transforms.ElasticTransform(alpha_range=(28.0, 30.0),\n",
    "                                       sigma_range=(3.5, 4.0),\n",
    "                                       p=0.3),\n",
    "        mt_transforms.RandomAffine(degrees=4.6,\n",
    "                                   scale=(0.98, 1.02),\n",
    "                                   translate=(0.03, 0.03)),\n",
    "        mt_transforms.RandomTensorChannelShift((-0.10, 0.10)),\n",
    "        mt_transforms.ToTensor(),\n",
    "        mt_transforms.NormalizeInstance(),\n",
    "    ])\n",
    "\n",
    "    # These are the validation/testing transformations\n",
    "    val_transform = transforms.Compose([\n",
    "        mt_transforms.CenterCrop2D((128, 128)),\n",
    "        mt_transforms.ToTensor(),\n",
    "        mt_transforms.NormalizeInstance(),\n",
    "    ])\n",
    "\n",
    "    # This code will iterate over the folders and load the data, filtering\n",
    "    # the slices without labels and then concatenating all the datasets together\n",
    "    train_datasets = []\n",
    "    for bids_ds in tqdm_notebook(context[\"bids_path_train\"], desc=\"Loading training set\"):\n",
    "        ds_train = BidsDataset(bids_ds,\n",
    "                               transform=train_transform,\n",
    "                               slice_filter_fn=SliceFilter())\n",
    "        train_datasets.append(ds_train)\n",
    "\n",
    "    ds_train = ConcatDataset(train_datasets)\n",
    "    print(f\"Loaded {len(ds_train)} axial slices for the training set.\")\n",
    "    train_loader = DataLoader(ds_train, batch_size=context[\"batch_size\"],\n",
    "                              shuffle=True, pin_memory=True,\n",
    "                              collate_fn=mt_datasets.mt_collate,\n",
    "                              num_workers=1)\n",
    "    \n",
    "    # Validation dataset ------------------------------------------------------\n",
    "    validation_datasets = []\n",
    "    for bids_ds in tqdm_notebook(context[\"bids_path_validation\"], desc=\"Loading validation set\"):\n",
    "        ds_val = BidsDataset(bids_ds,\n",
    "                             transform=val_transform,\n",
    "                             slice_filter_fn=SliceFilter())\n",
    "        validation_datasets.append(ds_val)\n",
    "\n",
    "    ds_val = ConcatDataset(validation_datasets)\n",
    "    print(f\"Loaded {len(ds_val)} axial slices for the validation set.\")\n",
    "    val_loader = DataLoader(ds_val, batch_size=context[\"batch_size\"],\n",
    "                            shuffle=True, pin_memory=True,\n",
    "                            collate_fn=mt_datasets.mt_collate,\n",
    "                            num_workers=0)\n",
    "    \n",
    "    \n",
    "    # Model definition ---------------------------------------------------------\n",
    "    model = Classifier(drop_rate=context[\"dropout_rate\"],\n",
    "                       bn_momentum=context[\"batch_norm_momentum\"])\n",
    "\n",
    "    model.cuda()\n",
    "\n",
    "    num_epochs = context[\"num_epochs\"]\n",
    "    initial_lr = context[\"initial_lr\"]\n",
    "\n",
    "    # Using SGD with cosine annealing learning rate\n",
    "    optimizer = optim.SGD(model.parameters(), lr=initial_lr)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n",
    "\n",
    "    # Write the metrics, images, etc to TensorBoard format\n",
    "    writer = SummaryWriter(log_dir=context[\"log_directory\"])\n",
    "    \n",
    "    # Binary Cross Entropy Loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    # Training loop -----------------------------------------------------------\n",
    "    best_validation_loss = float(\"inf\")\n",
    "    \n",
    "    lst_train_loss = []\n",
    "    lst_val_loss = []\n",
    "    lst_accuracy = []\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(1, num_epochs+1), desc=\"Training\"):\n",
    "        start_time = time.time()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        lr = scheduler.get_lr()[0]\n",
    "        writer.add_scalar('learning_rate', lr, epoch)\n",
    "\n",
    "        model.train()\n",
    "        train_loss_total = 0.0\n",
    "        num_steps = 0\n",
    "    \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            input_samples = batch[\"input\"]\n",
    "            input_labels = get_modality(batch)\n",
    "            \n",
    "            var_input = input_samples.cuda()\n",
    "            var_labels = torch.cuda.LongTensor(input_labels).cuda(non_blocking=True)\n",
    "\n",
    "            outputs = model(var_input)\n",
    "            \n",
    "            loss = criterion(outputs, var_labels)\n",
    "            train_loss_total += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            num_steps += 1\n",
    "            \n",
    "        train_loss_total_avg = train_loss_total / num_steps\n",
    "        lst_train_loss.append(train_loss_total_avg)\n",
    "        \n",
    "        #tqdm.write(f\"Epoch {epoch} training loss: {train_loss_total_avg:.4f}.\")\n",
    "        print(f\"Epoch {epoch} training loss: {train_loss_total_avg:.4f}.\")\n",
    "        \n",
    "        \n",
    "        # Validation loop -----------------------------------------------------\n",
    "        model.eval()\n",
    "        val_loss_total = 0.0\n",
    "        num_steps = 0\n",
    "        \n",
    "        val_accuracy = 0\n",
    "        num_samples = 0\n",
    "        \n",
    "        for i, batch in enumerate(val_loader):\n",
    "            input_samples = batch[\"input\"]\n",
    "            input_labels = get_modality(batch)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                var_input = input_samples.cuda()\n",
    "                var_labels = torch.cuda.LongTensor(input_labels).cuda(non_blocking=True)\n",
    "\n",
    "                outputs = model(var_input)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                loss = criterion(outputs, var_labels)\n",
    "                val_loss_total += loss.item()\n",
    "                \n",
    "                val_accuracy += int((var_labels == preds).sum())\n",
    "                \n",
    "            num_steps += 1\n",
    "            num_samples += context['batch_size']\n",
    "            \n",
    "        val_loss_total_avg = val_loss_total / num_steps\n",
    "        lst_val_loss.append(val_loss_total_avg)\n",
    "        \n",
    "        #tqdm.write(f\"Epoch {epoch} validation loss: {val_loss_total_avg:.4f}.\")\n",
    "        print(f\"Epoch {epoch} validation loss: {val_loss_total_avg:.4f}.\")\n",
    "        \n",
    "        val_accuracy_avg = 100 * val_accuracy / num_samples\n",
    "        lst_accuracy.append(val_accuracy_avg)\n",
    "        \n",
    "        #tqdm.write(f\"Epoch {epoch} accuracy : {val_accuracy_avg:.4f}.\")\n",
    "        print(f\"Epoch {epoch} accuracy : {val_accuracy_avg:.4f} %.\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        #tqdm.write(\"Epoch {} took {:.2f} seconds.\".format(epoch, total_time))\n",
    "        print(\"Epoch {} took {:.2f} seconds.\".format(epoch, total_time))\n",
    "        \n",
    "        \n",
    "        if val_loss_total_avg < best_validation_loss:\n",
    "            best_validation_loss = val_loss_total_avg\n",
    "            torch.save(model, \"./\"+context[\"log_directory\"]+\"/best_model.pt\")\n",
    "\n",
    "        \n",
    "    # save final model\n",
    "    torch.save(model, \"./\"+context[\"log_directory\"]+\"/final_model.pt\")\n",
    "    \n",
    "    # display and save the metrics\n",
    "    parameters = \"CrossEntropyLoss, batchsize=\" + str(context['batch_size'])\n",
    "    parameters += \", initial_lr=\" + str(context['initial_lr'])\n",
    "    parameters += \", dropout=\" + str(context['dropout_rate'])\n",
    "    \n",
    "    \n",
    "    plt.subplot(2,1,1)\n",
    "    plt.title(parameters)\n",
    "    plt.plot(lst_train_loss, color='red', label='Training')\n",
    "    plt.plot(lst_val_loss, color='blue', label='Validation')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(lst_accuracy)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    \n",
    "    plt.savefig(parameters+'.png')\n",
    "    plt.show()    \n",
    "    return\n",
    "\n",
    "\n",
    "def run_main(command):\n",
    "    with open('config_small.json') as fhandle:\n",
    "        context = json.load(fhandle)\n",
    "\n",
    "    #command = context[\"command\"]\n",
    "\n",
    "    if command == 'train':\n",
    "        cmd_train(context)\n",
    "        #shutil.copyfile(sys.argv[1], \"./\"+context[\"log_directory\"]+\"/config_file.json\")\n",
    "    elif command == 'test':\n",
    "        cmd_test(context)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb4acd72d7d445681385b1f86d11250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading training set', max=6, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3518 axial slices for the training set.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ed4b615d8547e2862b5307eb7ae519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading validation set', max=4, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2948 axial slices for the validation set.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019a9cc401404fb6ae39aae6d9be80a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=50, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training loss: 1.4957.\n",
      "Epoch 1 validation loss: 1.4231.\n",
      "Epoch 1 accuracy : 68.3784 %.\n",
      "Epoch 1 took 37.71 seconds.\n",
      "Epoch 2 training loss: 1.2851.\n",
      "Epoch 2 validation loss: 1.3445.\n",
      "Epoch 2 accuracy : 71.8243 %.\n",
      "Epoch 2 took 37.19 seconds.\n",
      "Epoch 3 training loss: 1.2125.\n",
      "Epoch 3 validation loss: 1.3092.\n",
      "Epoch 3 accuracy : 75.2027 %.\n",
      "Epoch 3 took 37.40 seconds.\n",
      "Epoch 4 training loss: 1.1815.\n",
      "Epoch 4 validation loss: 1.2939.\n",
      "Epoch 4 accuracy : 75.1689 %.\n",
      "Epoch 4 took 37.73 seconds.\n",
      "Epoch 5 training loss: 1.1624.\n",
      "Epoch 5 validation loss: 1.2612.\n",
      "Epoch 5 accuracy : 78.7838 %.\n",
      "Epoch 5 took 37.61 seconds.\n",
      "Epoch 6 training loss: 1.1462.\n",
      "Epoch 6 validation loss: 1.2526.\n",
      "Epoch 6 accuracy : 80.0676 %.\n",
      "Epoch 6 took 37.74 seconds.\n",
      "Epoch 7 training loss: 1.1396.\n",
      "Epoch 7 validation loss: 1.2255.\n",
      "Epoch 7 accuracy : 82.2297 %.\n",
      "Epoch 7 took 37.46 seconds.\n",
      "Epoch 8 training loss: 1.1300.\n",
      "Epoch 8 validation loss: 1.2319.\n",
      "Epoch 8 accuracy : 81.4527 %.\n",
      "Epoch 8 took 37.41 seconds.\n",
      "Epoch 9 training loss: 1.1214.\n",
      "Epoch 9 validation loss: 1.2116.\n",
      "Epoch 9 accuracy : 83.7500 %.\n",
      "Epoch 9 took 37.53 seconds.\n",
      "Epoch 10 training loss: 1.1175.\n",
      "Epoch 10 validation loss: 1.2298.\n",
      "Epoch 10 accuracy : 81.2838 %.\n",
      "Epoch 10 took 37.29 seconds.\n",
      "Epoch 11 training loss: 1.1083.\n",
      "Epoch 11 validation loss: 1.1963.\n",
      "Epoch 11 accuracy : 85.5405 %.\n",
      "Epoch 11 took 37.53 seconds.\n",
      "Epoch 12 training loss: 1.1029.\n"
     ]
    }
   ],
   "source": [
    "run_main('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy evaluation\n",
    "\n",
    "Since we only measured the accuracy over each slice we have to measure the accuracy for a whole acquisition comprising several (at leat 15) tranches. If we consider the label that appears the most among the acquisition we obtain the estimated label for the acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tirage():\n",
    "    x = np.random.rand()\n",
    "    if x <= 0.84:\n",
    "        return([0,0,0,0,0,1])\n",
    "    else:\n",
    "        i = np.random.randint(4)\n",
    "        l = [0 for i in range(6)]\n",
    "        l[i] = 1\n",
    "        return(l)\n",
    "    \n",
    "def montecarlo(n):\n",
    "    accuracy = 0\n",
    "    for i in range(n):\n",
    "        # we average over 15 slices\n",
    "        labels = [0 for i in range(6)]\n",
    "        for j in range(15):\n",
    "            t = tirage()\n",
    "            labels = [x+y for x,y in zip(labels, t)]\n",
    "        label = labels.index(max(labels))\n",
    "        if label == 5:\n",
    "            accuracy += 1\n",
    "    return(100 * accuracy/n)\n",
    "            \n",
    "montecarlo(10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we manage to obtain a promising 99.988% accuracy for each acquisition, which is enough for a reliable release."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config_small.json') as fhandle:\n",
    "    context = json.load(fhandle)\n",
    "\n",
    "command = context[\"command\"]\n",
    "\n",
    "if command == 'train':\n",
    "    cmd_train(context)\n",
    "    shutil.copyfile(sys.argv[1], \"./\"+context[\"log_directory\"]+\"/config_file.json\")\n",
    "elif command == 'test':\n",
    "    cmd_test(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds = torch.rand(18,6)\n",
    "preds_norm = nn.Softmax()\n",
    "labels = [4, 5, 1, 3, 2, 5, 2, 5, 0, 3, 1, 0, 4, 0, 3, 0, 5, 3]\n",
    "var_labels = []\n",
    "for l in labels :\n",
    "    a = [0 for i in range(6)]\n",
    "    a[l] = 1 \n",
    "    var_labels.append(a)\n",
    "var_labels = torch.FloatTensor(var_labels)\n",
    "\n",
    "\n",
    "\n",
    "CS_loss = nn.BCELoss()\n",
    "loss = CS_loss(var_labels, preds)\n",
    "\n",
    "NLL = nn.NLLLoss()\n",
    "loss = NLL(var_labels, preds)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    a = torch.rand(18,1,128,128)\n",
    "    a = a.cuda()\n",
    "    t = time.time()\n",
    "            \n",
    "    preds = model(a)\n",
    "\n",
    "    print(f\"took {time.time() - t} to compute\")\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ivado",
   "language": "python",
   "name": "ivado"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
